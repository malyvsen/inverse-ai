{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN training\n",
    "We have embeddings of over 1000 summaries of popular movies and a few hundred completely unrelated images. We'll train a GAN to generate images from movie embeddings, without any constraint on the relationship between the movie and the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from skimage.io import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(root_dir + 'data/movies/processed.csv')\n",
    "movie_vectors = np.array([np.fromstring(vector, sep=' ') for vector in movies['vector']])\n",
    "movie_vectors_std = movie_vectors.std()\n",
    "print(f'movie_vectors: shape {movie_vectors.shape}, std {movie_vectors_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PenabrancaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, noise_distribution=torch.distributions.Normal(0, .02)):\n",
    "        self.images = torch.tensor(np.load(data_path)).float()\n",
    "        self.noise_distribution = noise_distribution\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        result = self.images[idx]\n",
    "        if np.random.rand() < .5:\n",
    "            result = result.flip(-2)\n",
    "        if np.random.rand() < .5:\n",
    "            result = result.flip(-1)\n",
    "        if np.random.rand() < .5:\n",
    "            result = result.permute(1, 0)\n",
    "        result += self.noise_distribution.sample(result.shape)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def image_size(self):\n",
    "        return self.images.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = PenabrancaDataset(root_dir + 'data/penabranca/processed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('test image')\n",
    "plt.imshow(image_dataset[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = torch.utils.data.DataLoader(image_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is based on [this](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py) DCGAN code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.start_size = image_dataset.image_size // (2 * 2)\n",
    "        self.dense = torch.nn.Sequential(torch.nn.Linear(movie_vectors.shape[-1], 128 * self.start_size ** 2))\n",
    "        \n",
    "        def conv_block(in_filters, out_filters, upscaling):\n",
    "            return [\n",
    "                torch.nn.Upsample(scale_factor=upscaling),\n",
    "                torch.nn.Conv2d(in_filters, out_filters, 3, stride=1, padding=1),\n",
    "                torch.nn.BatchNorm2d(out_filters, 0.8),\n",
    "                torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "\n",
    "        self.conv_blocks = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            *conv_block(128, 128, upscaling=2),\n",
    "            *conv_block(128, 64, upscaling=2),\n",
    "            torch.nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, vector):\n",
    "        if torch.cuda.is_available():\n",
    "            vector = vector.cuda()\n",
    "        start = self.dense(vector)\n",
    "        start = start.view(start.shape[0], -1, self.start_size, self.start_size)\n",
    "        img = self.conv_blocks(start)\n",
    "        img = img.squeeze(1) # skimage grayscale format (no channel dimension)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_block(in_filters, out_filters, batch_norm=True):\n",
    "            block = [\n",
    "                torch.nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "                torch.nn.Dropout2d(0.25),\n",
    "            ]\n",
    "            if batch_norm:\n",
    "                block.append(torch.nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = torch.nn.Sequential(\n",
    "            *conv_block(1, 16, batch_norm=False),\n",
    "            *conv_block(16, 32),\n",
    "            *conv_block(32, 64),\n",
    "            *conv_block(64, 128),\n",
    "        )\n",
    "\n",
    "        downsampled_size = image_dataset.image_size // 2 ** 4\n",
    "        self.adv_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128 * downsampled_size ** 2, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, img):\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        img = img.unsqueeze(1) # add channel dimension\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        realness = self.adv_layer(out)\n",
    "        realness = realness.squeeze(1)\n",
    "        return realness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if 'Conv' in class_name:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif 'BatchNorm2d' in class_name:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('switching to cuda')\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    criterion.cuda()\n",
    "else:\n",
    "    print('cuda unavailable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, optimizer_params=None, num_epochs=64, epochs_per_preview=4):\n",
    "    if optimizer_params is None:\n",
    "        optimizer_params = {'lr': 2e-4, 'betas': (.5, .999)}\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), **optimizer_params)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), **optimizer_params)\n",
    "    \n",
    "    generator_losses = []\n",
    "    discriminator_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for image_batch in image_loader:\n",
    "            vectors = torch.normal(mean=0, std=movie_vectors_std, size=(len(image_batch), movie_vectors.shape[-1]))\n",
    "            fakes = generator(vectors)\n",
    "            fake_realness = discriminator(fakes)\n",
    "            real_realness = discriminator(image_batch)\n",
    "\n",
    "            zero_realness = torch.zeros_like(fake_realness)\n",
    "            full_realness = torch.ones_like(fake_realness)\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "            generator_loss = criterion(fake_realness, full_realness)\n",
    "            generator_losses.append(generator_loss.item())\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            discriminator_loss = (criterion(fake_realness, zero_realness) + criterion(real_realness, full_realness)) / 2\n",
    "            discriminator_losses.append(discriminator_loss.item())\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        if epoch % epochs_per_preview == 0:\n",
    "            plt.title(f'image generated in epoch {epoch}')\n",
    "            plt.imshow(fakes[0].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.show()\n",
    "    \n",
    "    return np.array(generator_losses), np.array(discriminator_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator_loss, discriminator_loss = train(generator, discriminator, num_epochs=1024, epochs_per_preview=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(generator_loss, label='generator loss')\n",
    "plt.plot(discriminator_loss, label='discriminator loss')\n",
    "plt.legend()\n",
    "plt.xlabel('batch number')\n",
    "plt.show)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training's done, let's save the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "for idx, vector in tqdm(enumerate(movie_vectors)):\n",
    "    generated = generator(torch.tensor(vector).float().unsqueeze(0))[0]\n",
    "    imsave(root_dir + f'generated/{idx}.png', generated.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse-ai-venv",
   "language": "python",
   "name": "inverse-ai-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
